---
title: Acceptance-rejection algorithm
author: Naveen Gabriel
date: '2020-12-18'
tags:
  - R
  - mcmc
  - sampling
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<p>This is one of my favourite introductory algorithm for sampling data from a distribution. A pre requisite basic knowledge about probability distribution, CDF and inverse CDF is necessary to follow the article.</p>
<p>A probability distribution gives the probability of having a random variable Y. To get Y from probability, a usual method is to find inverse CDF. There are several pdf whose inverse are difficult to obtain, does not exist or have no closed solution. Acceptance-Rejection method is used when it is not possible to find inverse CDF of a distribution, but we can get random variable (rv) using inverse of another similar distribution and say whether this particular rv can be used as if it was generated from our original distribution.</p>
<p>In acceptance rejection method, the idea is to find a probability distribution, <span class="math inline">\(g_y\)</span>, from which we can generate a random variable (rv) Y and able to tell whether this rv can be accepted for our target distribution, <span class="math inline">\(f_y\)</span>. The random variable Y is chosen in such a way that the <span class="math inline">\(g_y\)</span> can be scaled to majorize <span class="math inline">\(f_y\)</span> using some constant c; that is <span class="math inline">\(c.g_y(x)\geq f_y(x)\)</span>. The density <span class="math inline">\(g_y\)</span> is known as majorizing density or proposal density and <span class="math inline">\(f_y\)</span> is known as target density. The <a href="https://math.stackexchange.com/questions/2667060/support-of-density-function">support</a> of the target density must be contained in the support of proposal density. For density having infinite support, the majorizing density should also have infinite support. In the case of infinite support, it is critical that the majorizing density should not approach zero faster than the target density. It makes sense now as if there’s a region of the support of f that g can never touch, then that area will never get sampled.</p>
<p><strong>Following is the Acceptance Rejection algorithm :</strong></p>
<ol style="list-style-type: decimal">
<li>Sample a rv Y <span class="math inline">\(\sim g_y(x)\)</span> ( by taking inverse CDF. Remember, taking inverse of majorizing function should be easy)</li>
<li>Sample from uniform distribution U <span class="math inline">\(\sim\)</span> Unif(0,1).</li>
<li>Reject Y if U &gt; <span class="math inline">\(\frac{f_y(x)}{C.g_y(x)}\)</span>. Go to step 1.</li>
<li>Else accept Y for <span class="math inline">\(f_y(x)\)</span>.</li>
<li>Keep repeating the above step for desired number of samples.</li>
</ol>
<p>Step 2 generates uniform probability between 0 and 1. In Step 3 but the ratio <span class="math inline">\(f_y(x)/g_y(x)\)</span> is bounded by a constant c&gt;0. c.g(x) acts as a envelope for the target function <span class="math inline">\(f_y\)</span>. I did not find much meaning of the ratio but one way to think about this fraction is that, it inherently implies that how many fraction of rv for <span class="math inline">\(f_y\)</span> is included in c*<span class="math inline">\(g_y\)</span>. Step 3 implies that if the probability of generated point is lesser than probability generated by uniform distribution then accept that sample, which I find is a nice way to think about it. We need to maximize this fraction so that we can cover most of the points in <span class="math inline">\(g_y\)</span> for <span class="math inline">\(f_y\)</span>. Moreover, larger C value will result into rejection of more samples so we need to find c as small as possible.</p>
<p>Example-1: Generating beta(2,7) distribution</p>
<pre class="r"><code>#code by : Krzysztof Bartoszek
fgenbeta&lt;-function(c){
    x&lt;-NA
    num.reject&lt;-0
    while (is.na(x)){
    y&lt;-runif(1)
    u&lt;-runif(1)
    if (u&lt;=dbeta(y,2,7)/c){x&lt;-y}
    else{num.reject&lt;-num.reject+1}
    }
    c(x,num.reject)
}

y&lt;-dbeta(seq(0,2,0.001),2,7)
c&lt;-max(y)

mbetas1&lt;-sapply(rep(c,10000),fgenbeta)
mbetas2&lt;-sapply(rep(4,10000),fgenbeta)

par(mfrow=c(1,2))

hist(mbetas1[2,],col=&quot;black&quot;,breaks=100,xlab=&quot;&quot;,ylab=&quot;&quot;,freq=FALSE,main=&quot;&quot;);hist(mbetas2[2,],col=gray(0.8),breaks=100,xlab=&quot;&quot;,ylab=&quot;&quot;,freq=FALSE,main=&quot;&quot;,add=TRUE);legend(&quot;topright&quot;,pch=19,cex=1.2,legend=c(&quot;c=3.172&quot;,&quot;c=4&quot;),col=c(&quot;black&quot;,gray(0.8)),bty=&quot;n&quot;)
hist(mbetas1[1,],col=&quot;black&quot;,breaks=100,xlab=&quot;&quot;,ylab=&quot;&quot;,freq=FALSE,main=&quot;&quot;,ylim=c(0,3.5));hist(mbetas2[1,],col=gray(0.8),breaks=100,xlab=&quot;&quot;,ylab=&quot;&quot;,freq=FALSE,main=&quot;&quot;,add=TRUE);points(seq(0,2,0.001),y,pch=19,cex=0.3,col=gray(0.4));abline(h=3.172554,lwd=3);legend(&quot;right&quot;,pch=19,cex=1.2,legend=c(&quot;c=3.172&quot;,&quot;c=4&quot;),col=c(&quot;black&quot;,gray(0.8)),bty=&quot;n&quot;)</code></pre>
<p><img src="jey07.github.io/post/2020-12-18-acceptance-rejection-algorithm/index.en_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>We can see from the above example that the larger value of c tends to reject more sample than low value of c. The black color shows that the samples of beta distribution was accepted more than that of gray color. In second picture one can see black lines even though it is in background which shows that lesser c values has lower rejection rate.</p>
<p>As an another example, let’s generate samples for normal distribution <span class="math inline">\(\mathcal{N(0,1)}\)</span> using Laplace or double exponential distribution as proposal density DE(0, 1). Below are the given two distribution :</p>
<p><span class="math display">\[
\begin{aligned}
    g(x|\mu=0,\alpha=1) &amp;= \frac{1}{2}* e^{-|x|}  \hspace{40pt} :Laplace Distribution  \\
    f(x |\mu=0,\alpha=1) &amp;=  \frac{1}{\sqrt{2\pi}}* e^{\frac{-x^{2}}{2}} \hspace{24pt} :Normal Distribution \\
\end{aligned}
\]</span></p>
<p>As I mentioned before that the ratio <span class="math inline">\(f_y(x)/g_y(x)\)</span> is bounded by a constant c&gt;0 and <span class="math inline">\(sup_x{f(x)/g(x))}\)</span> <span class="math inline">\(\le\)</span> c. Another way to imagine the above equation is that, this equation gives the probability of accepting the sample and we need to maximize the acceptance sample. So,</p>
<p><span class="math display">\[\begin{aligned}
    C&amp;\geq \frac{f(x)}{g(x)} \\
    C&amp;\geq \frac{\sqrt{2}* e^{-\frac{x^2}{2}+|x|}}{\sqrt{\pi}}
\end{aligned}
\]</span></p>
<p>Then we differentiate the above ratio and equate to 0 to get the value of x which will give the value of C. On doing it, we get it as:</p>
<p><span class="math display">\[\frac{\sqrt{2}* e^{-\frac{x^2}{2}+|x|}}{\sqrt{\pi}} *(\frac{|x|}{x}-x) \]</span></p>
<p>Setting the above differential to zero we get the maximum value of above equation at x=1, value of C is obtained.</p>
<p><span class="math display">\[ C = \sqrt{\frac{2e}{\pi}} \]</span></p>
<p>The condition to accept RV generated from g(x) as RV for f(x) is :
<span class="math display">\[
\begin{aligned}
U &amp; \leq \frac{f(x)}{C*g(x)} \\
U &amp;\leq 0.5*e^{\frac{x^2}{2} + |x|}
\end{aligned}
\]</span></p>
<p>Step 1 of equation is generating samples from Laplace distribution whose inverse can be found. This <a href="https://math.stackexchange.com/questions/2667060/support-of-density-function">link</a> gives how to find the inverse.</p>
<p>Below is the complete code for acceptance-rejection:</p>
<pre class="r"><code>#author: Naveen gabriel
accept_reject &lt;- function(sam) {
    f_x &lt;- c()
    cnt &lt;- 1
    while(cnt&lt;=sam){ 
        U &lt;- runif(1,0,1)    
        r_x &lt;- ifelse(U&lt;0.5, log(2*U), -log(2-(2*U)))  #generate a random variable from laplace distribution
        
        uni &lt;- runif(1)
        frac &lt;- exp(-(r_x^2)/2 + abs(r_x) - 0.5)  #value of f(X)/c(g(X))
    
        if(uni&lt;=frac){
            f_x[cnt]&lt;- r_x
            cnt &lt;- cnt+1
        }
        total &lt;&lt;- total + 1
    }
    
return(f_x)
}

n = 2000
total &lt;-0
values &lt;- accept_reject(n)
normal_dist &lt;- rnorm(2000,0,1)

compare_data &lt;- cbind(values,normal_dist)
compare_data &lt;- as.data.frame(compare_data)

colnames(compare_data) &lt;- c(&quot;Accept_Rejection&quot;,&quot;normal_dist&quot;)</code></pre>
<p>We can see that the normal distribution generated by acceptance rejection method is nearly same as distribution generated by rnorm.</p>
<pre class="r"><code>ggplot(compare_data) + geom_histogram(aes(Accept_Rejection,fill=&quot;Accept_Rejection&quot;),alpha=0.4) + geom_histogram(aes(normal_dist,fill =&quot;normal_dist&quot;), alpha =0.5) + labs(title=&quot;Histograms of two distribution&quot;, x = &quot;Histograms&quot;)</code></pre>
<p><img src="jey07.github.io/post/2020-12-18-acceptance-rejection-algorithm/index.en_files/figure-html/hist-normal-exponential-1.png" width="768" /></p>
<p>One property of the rejection sampling algorithm is that the number of draws we need to take from the candidate density g before we accept a candidate is a geometric random variable with success probability 1/c. Hence, the expected rejection rate is equal to:
<span class="math display">\[1 - \frac{1}{c} = 0.2398264\]</span></p>
<p><br>
The average rejection rate is :
<span class="math display">\[1 - \frac{2000}{total}\]</span>
<br></p>
<p>Where total is the total number of iterations required to generate 2000 samples. Our average rejection rate is nearly equal to expected rejection rate.</p>
<pre><code>## The average rejection rate is : 0.245283</code></pre>
<p>You might be thinking why we need sampling method, that too generating random samples from probability. Imagine you have dataset where there are missing values and you might know a distribution from which the samples can be generated. But taking the inverse of that distribution might be too difficult. In that case we can use this method. It might feel weird to do this process because we don’t encounter it while doing sampling. Many known densities (exponential, normal, beta etc) have library methods that generate samples using one of different sampling method. Unlike inverse CDF, this method can be extend to multivariate random variables. If the random variable is of a reasonably low dimension (less than 10?), then rejection sampling is a plausible general approach. But keep in mind that as the number of dimension increase the rate of rejection increases.</p>
<p>There are different flavors of rejection sampling like Squeezed rejection sampling, adaptive rejection sampling etc. The acceptance-rejection method can be generalized to the Metropolis–Hastings algorithm which is a type of Markov chain Monte Carlo simulation. Below are some of the awesome link to understand more of this method:</p>
<ul>
<li><a href="http://www.columbia.edu/~ks20/4703-Sigman/4703-07-Notes-ARM.pdf">Columbia university notes</a></li>
<li><a href="https://stats.stackexchange.com/questions/391598/understanding-rejection-sampling">Stack overflow</a></li>
<li>Rejection sampling (RS) technique, suggested first by <a href="https://dornsifecms.usc.edu/assets/sites/520/docs/VonNeumann-ams12p36-38.pdf">John von Neumann</a> in 1951.</li>
</ul>
